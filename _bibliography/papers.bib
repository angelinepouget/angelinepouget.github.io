---
---

@article{pouget2024filter,
  title={No Filter: Cultural and Socioeconomic Diversity in Contrastive Vision-Language Models},
  author={Pouget, Angéline and Beyer, Lucas and Bugliarello, Emanuele and Wang, Xiao and Steiner, Andreas and Zhai, Xiaohua and Alabdulmohsin, Ibrahim},
  abbr={NeurIPS 2024},
  year={2024},
  pdf={https://arxiv.org/pdf/2405.13777},
  arxiv={2405.13777},
  abstract={We study cultural and socioeconomic diversity in contrastive vision-language models (VLMs). Using a broad range of benchmark datasets and evaluation metrics, we bring to attention several important findings. First, the common filtering of training data to English image-text pairs disadvantages communities of lower socioeconomic status and negatively impacts cultural understanding. Notably, this performance gap is not captured by -- and even at odds with -- the currently popular evaluation metrics derived from the Western-centric ImageNet and COCO datasets. Second, pretraining with global, unfiltered data before fine-tuning on English content can improve cultural understanding without sacrificing performance on said popular benchmarks. Third, we introduce the task of geo-localization as a novel evaluation metric to assess cultural diversity in VLMs. Our work underscores the value of using diverse data to create more inclusive multimodal systems and lays the groundwork for developing VLMs that better represent global perspectives.},
  selected={true}
}

@article{pouget2024drawing,
  title={Back to the Drawing Board for Fair Representation Learning},
  author={Angéline Pouget and Nikola Jovanović and Mark Vero and Robin Staab and Martin Vechev},
  abbr={Preprint},
  year={2024},
  pdf={https://arxiv.org/pdf/2405.18161},
  arxiv={2405.18161},
  abstract={The goal of Fair Representation Learning (FRL) is to mitigate biases in machine learning models by learning data representations that enable high accuracy on downstream tasks while minimizing discrimination based on sensitive attributes. The evaluation of FRL methods in many recent works primarily focuses on the tradeoff between downstream fairness and accuracy with respect to a single task that was used to approximate the utility of representations during training (proxy task). This incentivizes retaining only features relevant to the proxy task while discarding all other information. In extreme cases, this can cause the learned representations to collapse to a trivial, binary value, rendering them unusable in transfer settings. In this work, we argue that this approach is fundamentally mismatched with the original motivation of FRL, which arises from settings with many downstream tasks unknown at training time (transfer tasks). To remedy this, we propose to refocus the evaluation protocol of FRL methods primarily around the performance on transfer tasks. A key challenge when conducting such an evaluation is the lack of adequate benchmarks. We address this by formulating four criteria that a suitable evaluation procedure should fulfill. Based on these, we propose TransFair, a benchmark that satisfies these criteria, consisting of novel variations of popular FRL datasets with carefully calibrated transfer tasks. In this setting, we reevaluate state-of-the-art FRL methods, observing that they often overfit to the proxy task, which causes them to underperform on certain transfer tasks. We further highlight the importance of task-agnostic learning signals for FRL methods, as they can lead to more transferrable representations.},
  selected={true}
}

@article{hersche2023factorizers,
  title={Factorizers for Distributed Sparse Block Codes},
  author={Hersche, Michael and Terzic, Aleksandar and Karunaratne, Geethan and Langenegger, Jovin and Pouget, Ang{\'e}line and Cherubini, Giovanni and Benini, Luca and Sebastian, Abu and Rahimi, Abbas},
  journal={Neurosymbolic Artificial Intelligence Journal},
  abbr={NAI Journal},
  year={2024},
  pdf={https://neurosymbolic-ai-journal.com/system/files/nai-paper-713.pdf},
  arxiv={2303.13957},
  abstract={Distributed sparse block codes (SBCs) exhibit compact representations for encoding and manipulating symbolic data
  structures using fixed-with vectors. One major challenge however is to disentangle, or factorize, such data structures into their
  constituent elements without having to search through all possible combinations. This factorization becomes more challenging
  when queried by noisy SBCs wherein symbol representations are relaxed due to perceptual uncertainty and approximations
  made when modern neural networks generate the query vectors. To address these challenges, we first propose a fast and highly
  accurate method for factorizing a more flexible and hence generalized form of SBCs, dubbed GSBCs. Our iterative factorizer
  introduces a threshold-based nonlinear activation, conditional random sampling, and an ℓ∞-based similarity metric. Its random
  sampling mechanism, in combination with the search in superposition, allows us to analytically determine the expected number
  of decoding iterations, which matches the empirical observations up to the GSBC’s bundling capacity. Secondly, the proposed
  factorizer maintains a high accuracy when queried by noisy product vectors generated using deep convolutional neural networks
  (CNNs). This facilitates its application in replacing the large fully connected layer (FCL) in CNNs, whereby C trainable class
  vectors, or attribute combinations, can be implicitly represented by our factorizer having F-factor codebooks, each with √F
  C fixed codevectors. We provide a methodology to flexibly integrate our factorizer in the classification layer of CNNs with a novel loss
  function. With this integration, the convolutional layers can generate a noisy product vector that our factorizer can still decode,
  whereby the decoded factors can have different interpretations based on downstream tasks. We demonstrate the feasibility of our
  method on four deep CNN architectures over CIFAR-100, ImageNet-1K, and RAVEN datasets. In all use cases, the number of
  parameters and operations are notably reduced compared to the FCL.},
  selected={true}
}

@inproceedings{pouget2021fast,
  title={Fast and Accurate Camera Scene Detection on Smartphones},
  author={Pouget, Angéline and Ramesh, Sidharth and Giang, Maximilian and Chandrapalan, Ramithan and Tanner, Toni and Prussing, Moritz and Timofte, Radu and Ignatov, Andrey},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
  pages={2569--2580},
  abbr={CVPR Workshop},
  month={June},
  year={2021},
  pdf={https://openaccess.thecvf.com/content/CVPR2021W/MAI/papers/Pouget_Fast_and_Accurate_Camera_Scene_Detection_on_Smartphones_CVPRW_2021_paper.pdf},
  arxiv={2105.07869},
  abstract={AI-powered automatic camera scene detection mode is nowadays available in nearly any modern smartphone,
  though the problem of accurate scene prediction has not yet been addressed by the research community. This paper
  for the first time carefully defines this problem and proposes a novel Camera Scene Detection Dataset (CamSDD)
  containing more than 11K manually crawled images belonging to 30 different scene categories. We propose an efficient
  and NPU-friendly CNN model for this task that demonstrates a top-3 accuracy of 99.5% on this dataset and achieves more
  than 200 FPS on the recent mobile SoCs. An additional in-the-wild evaluation of the obtained solution is performed to
  analyze its performance and limitation in the real-world scenarios. The dataset and pre-trained models used in this paper
  are available on the project website.}
}

@inproceedings{ignatov2021fast,
  title={Fast Camera Image Denoising on Mobile GPUs With Deep Learning, Mobile AI 2021 Challenge: Report},
  author={Ignatov, Andrey and Byeoung-su, Kim and Timofte, Radu and Pouget, Angéline},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
  pages={2515--2524},
  abbr={CVPR Workshop},
  month={June},
  year={2021},
  pdf={https://openaccess.thecvf.com/content/CVPR2021W/MAI/papers/Ignatov_Fast_Camera_Image_Denoising_on_Mobile_GPUs_With_Deep_Learning_CVPRW_2021_paper.pdf},
  arxiv={2105.08629},
  abstract={Image denoising is one of the most critical problems in mobile photo processing. While many solutions have
  been proposed for this task, they are usually working with synthetic data and are too computationally expensive to run on
  mobile devices. To address this problem, we introduce the first Mobile AI challenge, where the target is to develop an
  end-to-end deep learning-based image denoising solution that can demonstrate high efficiency on smartphone GPUs. For this,
  the participants were provided with a novel large-scale dataset consisting of noisy-clean image pairs captured in the wild.
  The runtime of all models was evaluated on the Samsung Exynos 2100 chipset with a powerful Mali GPU capable of accelerating
  floating-point and quantized neural networks. The proposed solutions are fully compatible with any mobile GPU and are capable
  of processing 480p resolution images under 40-80 ms while achieving high fidelity results. A detailed description of all
  models developed in the challenge is provided in this paper.}
}