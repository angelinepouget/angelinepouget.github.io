---
---

@article{pouget2024filter,
  title={No Filter: Cultural and Socioeconomic Diversity in Contrastive Vision-Language Models},
  author={Angéline Pouget and Lucas Beyer and Emanuele Bugliarello and Xiao Wang and Andreas Peter Steiner and Xiaohua Zhai and Ibrahim Alabdulmohsin},
  abbr={Preprint},
  year={2024},
  pdf={https://arxiv.org/pdf/2405.13777},
  arxiv={2405.13777},
  abstract={We study cultural and socioeconomic diversity in contrastive vision-language models (VLMs). Using a broad range of benchmark datasets and evaluation metrics, we bring to attention several important findings. First, the common filtering of training data to English image-text pairs disadvantages communities of lower socioeconomic status and negatively impacts cultural understanding. Notably, this performance gap is not captured by -- and even at odds with -- the currently popular evaluation metrics derived from the Western-centric ImageNet and COCO datasets. Second, pretraining with global, unfiltered data before fine-tuning on English content can improve cultural understanding without sacrificing performance on said popular benchmarks. Third, we introduce the task of geo-localization as a novel evaluation metric to assess cultural diversity in VLMs. Our work underscores the value of using diverse data to create more inclusive multimodal systems and lays the groundwork for developing VLMs that better represent global perspectives.},
  selected={true}
}

@article{hersche2023factorizers,
  title={Factorizers for Distributed Sparse Block Codes},
  author={Hersche, Michael and Terzic, Aleksandar and Karunaratne, Geethan and Langenegger, Jovin and Pouget, Ang{\'e}line and Cherubini, Giovanni and Benini, Luca and Sebastian, Abu and Rahimi, Abbas},
  journal={Neurosymbolic Artificial Intelligence Journal},
  abbr={NAI Journal},
  year={2024},
  pdf={https://neurosymbolic-ai-journal.com/system/files/nai-paper-713.pdf},
  arxiv={2303.13957},
  abstract={Distributed sparse block codes (SBCs) exhibit compact representations for encoding and manipulating symbolic data
  structures using fixed-with vectors. One major challenge however is to disentangle, or factorize, such data structures into their
  constituent elements without having to search through all possible combinations. This factorization becomes more challenging
  when queried by noisy SBCs wherein symbol representations are relaxed due to perceptual uncertainty and approximations
  made when modern neural networks generate the query vectors. To address these challenges, we first propose a fast and highly
  accurate method for factorizing a more flexible and hence generalized form of SBCs, dubbed GSBCs. Our iterative factorizer
  introduces a threshold-based nonlinear activation, conditional random sampling, and an ℓ∞-based similarity metric. Its random
  sampling mechanism, in combination with the search in superposition, allows us to analytically determine the expected number
  of decoding iterations, which matches the empirical observations up to the GSBC’s bundling capacity. Secondly, the proposed
  factorizer maintains a high accuracy when queried by noisy product vectors generated using deep convolutional neural networks
  (CNNs). This facilitates its application in replacing the large fully connected layer (FCL) in CNNs, whereby C trainable class
  vectors, or attribute combinations, can be implicitly represented by our factorizer having F-factor codebooks, each with √F
  C fixed codevectors. We provide a methodology to flexibly integrate our factorizer in the classification layer of CNNs with a novel loss
  function. With this integration, the convolutional layers can generate a noisy product vector that our factorizer can still decode,
  whereby the decoded factors can have different interpretations based on downstream tasks. We demonstrate the feasibility of our
  method on four deep CNN architectures over CIFAR-100, ImageNet-1K, and RAVEN datasets. In all use cases, the number of
  parameters and operations are notably reduced compared to the FCL.},
  selected={true}
}

@inproceedings{pouget2021fast,
  title={Fast and Accurate Camera Scene Detection on Smartphones},
  author={Pouget, Ang{\'e}line and Ramesh, Sidharth and Giang, Maximilian and Chandrapalan, Ramithan and Tanner, Toni and Prussing, Moritz and Timofte, Radu and Ignatov, Andrey},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
  pages={2569--2580},
  abbr={CVPR Workshop},
  month={June},
  year={2021},
  pdf={https://openaccess.thecvf.com/content/CVPR2021W/MAI/papers/Pouget_Fast_and_Accurate_Camera_Scene_Detection_on_Smartphones_CVPRW_2021_paper.pdf},
  arxiv={2105.07869},
  abstract={AI-powered automatic camera scene detection mode is nowadays available in nearly any modern smartphone,
  though the problem of accurate scene prediction has not yet been addressed by the research community. This paper
  for the first time carefully defines this problem and proposes a novel Camera Scene Detection Dataset (CamSDD)
  containing more than 11K manually crawled images belonging to 30 different scene categories. We propose an efficient
  and NPU-friendly CNN model for this task that demonstrates a top-3 accuracy of 99.5% on this dataset and achieves more
  than 200 FPS on the recent mobile SoCs. An additional in-the-wild evaluation of the obtained solution is performed to
  analyze its performance and limitation in the real-world scenarios. The dataset and pre-trained models used in this paper
  are available on the project website.}
}

@inproceedings{ignatov2021fast,
  title={Fast Camera Image Denoising on Mobile GPUs With Deep Learning, Mobile AI 2021 Challenge: Report},
  author={Ignatov, Andrey and Byeoung-su, Kim and Timofte, Radu and Pouget, Ang{\'e}line},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
  pages={2515--2524},
  abbr={CVPR Workshop},
  month={June},
  year={2021},
  pdf={https://openaccess.thecvf.com/content/CVPR2021W/MAI/papers/Ignatov_Fast_Camera_Image_Denoising_on_Mobile_GPUs_With_Deep_Learning_CVPRW_2021_paper.pdf},
  arxiv={2105.08629},
  abstract={Image denoising is one of the most critical problems in mobile photo processing. While many solutions have
  been proposed for this task, they are usually working with synthetic data and are too computationally expensive to run on
  mobile devices. To address this problem, we introduce the first Mobile AI challenge, where the target is to develop an
  end-to-end deep learning-based image denoising solution that can demonstrate high efficiency on smartphone GPUs. For this,
  the participants were provided with a novel large-scale dataset consisting of noisy-clean image pairs captured in the wild.
  The runtime of all models was evaluated on the Samsung Exynos 2100 chipset with a powerful Mali GPU capable of accelerating
  floating-point and quantized neural networks. The proposed solutions are fully compatible with any mobile GPU and are capable
  of processing 480p resolution images under 40-80 ms while achieving high fidelity results. A detailed description of all
  models developed in the challenge is provided in this paper.}
}